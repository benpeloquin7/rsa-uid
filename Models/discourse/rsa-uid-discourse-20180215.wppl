// Globals
var rData = [{'nUtterances':200, 'alpha':4, 'resultType':'contextAwareSpeaker', 'theta':0.1}]

var getNUtterances = function(rData) {
  var data = rData[0]
  return data['nUtterances']
}

var getResultType = function(rData) {
  var data = rData[0]
  return data['resultType']
}

var getAlpha = function(rData) {
  var data = rData[0]
  return data['alpha']
}

var getTheta = function(rData) {
  var data = rData[0]
  return data['theta']
}

var nUtterances = getNUtterances(rData)
var resultType = getResultType(rData)
var alpha = getAlpha(rData)
var theta = getTheta(rData)

var d0 = ['a']
var d1 = ['a', 'b', 'b']
var d2 = ['a', 'b', 'b', 'b', 'a', 'b', 'b', 'b']

// Topics
var topicNames = ['t_distr1', 't_distr2', 't_distr3']
var topicNamesDistr = Categorical({vs:topicNames, ps:[0.01, 0.1, 1]})
var topicNamesPrior = function() { 
  return sample(topicNamesDistr)
}

// Worlds
var worlds = [{d:'a'}, {d:'b'}, {d:'c'}]
var wordsDistr = Categorical({vs:worlds, ps:[1, 1, 1]})
var worldsPrior = function() {return sample(wordsDistr)}

// Utterances
// var utterances = ['a', 'b', 'c', 'X a', 'X b', 'X c']
var utterances = ['a', 'b', 'c']
// var utterancesDistr = Categorical({vs:utterances, ps:[1, 1, 1, 0.5, 0.5, 0.5]})
var utterancesDistr = Categorical({vs:utterances, ps:[1, 1, 1]})
var utterancesPrior = function() {return sample(utterancesDistr)}

// Worlds cond. on topics
var t_distr1 = Categorical({vs:worlds, ps:[4, 2, 1]})
var t_distr2 = Categorical({vs:worlds, ps:[6, 2, 1]}) 
var t_distr3 = Categorical({vs:worlds, ps:[1, 1, 2]})
var topicDistr = {
  't_distr1': t_distr1,
  't_distr2': t_distr2,
  't_distr3': t_distr3
}
var getTopicNameDistr = function(topicName) {
  return topicDistr[topicName]
}

// Utterances

var litMeaning = function(utterance, world) {
  return utterance == world['d']
}

// Observed data
var observedData0 = ['a']
var observedData1 = ['a', 'a', 'b']
var observedData2 = ['a', 'a', 'b', 'a', 'a', 'b', 'a']

var utteranceMeaning = function(utterance) {
  if (utterance == '') {
    console.log("Error on", utterance)
  }
  var lastCharIndex = utterance.length - 1
  if (utterance.includes('a')) {
    return 'a'
  } else if (utterance.includes('b')) {
    return 'b'
  } else if (utterance.includes('c')) {
    return 'c'
  } else if (utterance.includes('d')) {
    return 'd'
  }else {
    console.log("Error on", utterance)
  }
}

var sampleWorldFromTopic = function(topicName) {
  return function() {
    if (topicName == 't_distr1') {
      return sample(t_distr1)
    } 
    else if (topicName == 't_distr2') { 
      return sample(t_distr2)
    } 
    else if (topicName == 't_distr3') { 
      return sample(t_distr3)
    } 
    else {
      error('Error')
    }
  }
}

var utteranceMaker = function(topicDistrName) {
  var world = sampleWorldFromTopic(topicDistrName)
  var utterance = utterancesPrior()
  return utterance
}

var cost = function(utterance) {
  return utterance.split(' ').length
}

//
// Noise
//
var _removeEmptyItems = function(arr) {
  filter(function(x) {return x != ''}, arr)
}

var replaceIndex = function(arr, index, replacement) {
  return arr.slice(0, index).concat([replacement]).concat(arr.slice(index + 1, arr.length))
}
var transformUtterance = function(transformFn) {
  return function(utterance) {
      var tokens = utterance.split(' ')
      var index = sample(RandomInteger({n:tokens.length}))
      var replacement = transformFn(index)
      return _removeEmptyItems(replaceIndex(tokens, index, replacement)).join(' ')
  }  
}
var _deleteFn = function(s) {
  return ''
}
var deleteTransform = transformUtterance(_deleteFn)

var vacuousNoiseModel = function(utterance) {
  return utterance
}
var deleteNoiseModel = function(utterance) {
  return flip(theta) ? deleteTransform(utterance) : utterance
}

var decode = cache(function(utteranceProduced, noiseModel) {
  Infer({
    model() {
      var utteranceIntended = utterancesPrior()
      var noisyUtterance = noiseModel(utteranceIntended, theta)
      factor(utteranceProduced == noisyUtterance ? 0 : -Infinity)
      return utteranceIntended
    }
  })
})


var L0 = function(utterance) {
  Infer({
    model() {
      var world = worldsPrior()
      var meaning = litMeaning(utteranceMeaning(utterance), world)
      factor(meaning ? 0 : -Infinity)
      return world
    }
  })
}

var S1 = function(topic) {
  Infer({
    model(){
      var world = sampleWorldFromTopic(topic)()
      var utterance = utterancesPrior()
      var L = L0(utterance)
      factor(alpha * L.score(world))
      return utterance
    }
  })
}

var listenerTopicDistrPosterior = function(observedData) {
  return Infer({method: 'enumerate'}, function() {
    var topicDistrName = topicNamesPrior()
    if (observedData != []) {
      var obsFn = function(datum){condition(datum == sample(S1(topicDistrName)))}
      mapData({data: observedData}, obsFn)
    }
    return topicDistrName
  })
}

var L1 = cache(function(utterance, data) {
  Infer({
    model() {
      // Sample a topic distr given prev utterances (data)
      var estTopicDistrName = sample(listenerTopicDistrPosterior(data))
      var estWorld = sampleWorldFromTopic(estTopicDistrName)()
      // Given est. topic distribution sample an utterance
//       var intendedUtterance = sample(decode(utterance, deleteNoiseModel))
      var meaning = litMeaning(utteranceMeaning(utterance), estWorld)
      factor(meaning ? 0 : -Infinity)
      // Listener jointly reasons about (meaning, topic)
      return {'estWorld': estWorld, 'estTopicDistrName': estTopicDistrName}
    }
  })
})

var S2 = cache(function(T, data) {
  Infer({
    model() {
      var world = sampleWorldFromTopic(T)()
      var utterance = utterancesPrior()
//       var intendedUtterance = utterancesPrior()
//       var producedUtterance = deleteNoiseModel(intendedUtterance)
      var L = L1(utterance, data)
      factor(alpha * (L.score({'estWorld': world, 'estTopicDistrName': T}) - cost(utterance)/10))
      // Note: the speaker likely shouldn't reasons explicitly about thhe
      // produced utterance
      return {'utterance': utterance}
    }
  })
})

//
// Speaker utterance generation
//
var d = []
var speakerFn = function(d) {
  var T = 't_distr1'
//   var w = sampleWorldFromTopic(T)()
  return d.concat([sample(Speaker(T, d))['producedUtterance']])
}

var speakerRecurse = function(d, n) {
  if (n == 0) {
    return d
  } else {
    return speakerRecurse(speakerFn(d.slice(0, d.length)), n-1)
  }
}

//
// Data
//
// Run speaker for nUtterances
// var contextAwareSpeaker = speakerRecurse([], nUtterances)

// speaker baseline1 - speaker talks without being aware of listener updates
// var contextUnawareSpeaker = repeat(nUtterances, (function(x) {sample(Speaker('t_distr1', []))}))

// non-speaker baseline2 - sample directly from distribution
// var baseline = repeat(nUtterances, (function(x) {sample(t_distr1)}))


//
// Helpers
//
// Generate a sequence of numbers
var plusOne = function(n) {
  return n + 1
}

var range_ = function(start, end) {
  if (start == end) {
    return end
  } else {
    return [start].concat(range_(start+1, end))
  }
}
var range = function(start, end) {
  var res = range_(start, end)
  return res.slice(0, res.length-1)
}

// Listener probability assignment to topic "t_distr1"
var getDataSizes = function(n) {
  return range(0, n)
}
// List of slice sizes
var dataSizes = getDataSizes(nUtterances)
// A context slice is the size of the preceding words (e.g. 0-3, 0-10, 0-n)
var contextSlices = map(function(x) {return [0, x]}, dataSizes)

// getListenerInferences
// =====================
// Given a slice of speaker utterance data, return listeners state of belief
// about the intended topic distribution.
//
var getListenerInferences = function(utterances) {
  var res = map(function(x) {
    return {
      "t_distr1": Math.exp(Listener('a', utterances.slice(x[0], x[1])).score([{d:'a'}, "t_distr1"])),
      "t_distr2": Math.exp(Listener('a', utterances.slice(x[0], x[1])).score([{d:'a'}, "t_distr2"])),
      "t_distr3": Math.exp(Listener('a', utterances.slice(x[0], x[1])).score([{d:'a'}, "t_distr3"]))}
  }, contextSlices)
  return res
}

// processUtterances
// =================
// Return a list of dictionaries containing speaker utterance data
// and corresponding state of listener inferences about topics
//
var processUtterances = function(speakerData, listenerInferences) {
  var dMap = map(function(x) {
    return {
      'utteranceNum': x, 
      'utterance': speakerData[x], 
      't_distr1': listenerInferences[x]['t_distr1'],
      't_distr2': listenerInferences[x]['t_distr2'],
      't_distr3': listenerInferences[x]['t_distr3']}
  }, dataSizes)
  return dMap
}

// runFn
// =====
// Return data of speaker utterances / listener inferences
//
// Parameters
// ----------
// resultType : str
//   One of (contextAwareSpeaker, contextUnawareSpeaker, baseline)
//
// Returns
// --------
// List of dicts
//
var runFn = function(resultType) {
  if (resultType == 'contextAwareSpeaker') {
    var contextAwareSpeaker = speakerRecurse([], nUtterances)
    var listenerInferences = getListenerInferences(contextAwareSpeaker)
    return processUtterances(contextAwareSpeaker, listenerInferences)
  } else if (resultType == 'contextUnawareSpeaker') {
    var contextUnawareSpeaker = repeat(nUtterances, (function(x) {
      var T = 't_distr1'
      var world = sampleWorldFromTopic(t)()
      sample(Speaker(T, []))}))
    var listenerInferences = getListenerInferences(contextUnawareSpeaker)
    return processUtterances(contextUnawareSpeaker, listenerInferences)
  } else if (resultType == 'baseline') {
    var baseline = repeat(nUtterances, (function(x) {utterancesPrior()}))
    var listenerInferences = getListenerInferences(baseline)
    return processUtterances(baseline, listenerInferences)
  } else {
    error('Bad resultType:', resultType)
  }
}


listenerTopicDistrPosterior([])
// listenerTopicDistrPosterior(['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'])

// viz.table(Listener('a', []))
// viz.table(Listener('X', []))
// viz.table(Listener('b', []))
// viz.table(Listener('c', []))



// marginalize(Speaker('t_distr1', []), function(x) {return x.producedUtterance})
// L1('a', ['a', 'b', 'c', 'b', 'b', 'a', 'c'])
S2('t_distr1', ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a'])