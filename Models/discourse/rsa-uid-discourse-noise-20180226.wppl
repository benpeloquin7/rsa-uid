/***

  rData
  alpha
  lambda
  theta
  resultType
  nUtterances
  targetDistr

***/

/*********************/
/*********************/
/******* SETUP *******/
/*********************/
/*********************/

var getNUtterances = function(rData) {
  var data = rData[0]
  return data['nUtterances']
}

var getAlpha = function(rData) {
  var data = rData[0]
  return data['alpha']
}

var getLambda = function(rData) {
  var data = rData[0]
  return data['lambda']
}

var getTheta = function(rData) {
  var data = rData[0]
  return data['theta']
}

var getModelName = function(rData) {
  var data = rData[0]
  return data['modelName']
}

var getN = function(rData) {
  var data = rData[0]
  return data['n']
}

var getSpeakerInput = function(rData) {
  var data = rData[0]
  return {d: data['speakerInput']}
}

var getTargetDistr = function(rData) {
  var data = rData[0]
  return data['targetDistr']
}

var getResultType = function(rData) {
  var data = rData[0]
  return data['resultType']
}

var rData = [{'alpha':10, 
              'lambda':20, 
              'modelName':'S1', 
              'theta':0.1, 
              'n':10000, 
              'targetDistr':'T1',
              'nUtterances':180,
              'resultType':'contextAwareS2'}]

var nUtterances = getNUtterances(rData)
var LAMBDA = getLambda(rData)
var ALPHA = getAlpha(rData)
var THETA = getTheta(rData)
var N = getN(rData)
var INPUT = getSpeakerInput(rData)
var targetDistr = getTargetDistr(rData)
var resultType = getResultType(rData)


var utterances = ['X a', 'a', 'X b', 'b', 'X c', 'c']
var utteranceProbs = [0.5, 1, 0.5, 1, 0.5, 1]
var utterancesDistr = Categorical({vs:utterances, ps:utteranceProbs})
var utterancesPrior = function() {return sample(utterancesDistr)}

var worlds = [{d:'a'}, {d:'b'}, {d:'c'}]
var worldProbs = [1, 1, 1]
var worldsDistr = Categorical({vs:worlds, ps:worldProbs})
var worldsPrior = function() {return sample(worldsDistr)}

var topicDistr = Categorical({vs:['T1', 'T2', 'T3'], ps:[1, 1, 1]})
var T1 = Categorical({vs:worlds, ps:[1, 0.5, 0.2]})
var T2 = Categorical({vs:worlds, ps:[0.2, 1, 0.5]}) 
var T3 = Categorical({vs:worlds, ps:[0.5, 0.2, 1]})
var topics = {
  'T1': T1,
  'T2': T2,
  'T3': T3,
}
var worldsGivenTopic = function(topic) {
   return sample(topics[topic])
}

/*********************/
/*********************/
/****** MODELS *******/
/*********************/
/*********************/

// Noise
var _removeEmptyItems = function(arr) {
  filter(function(x) {return x != ''}, arr)
}

var replaceIndex = function(arr, index, replacement) {
  return arr.slice(0, index).concat([replacement]).concat(arr.slice(index + 1, arr.length))
}
var transformUtterance = function(transformFn) {
  return function(utterance) {
      var tokens = utterance.split(' ')
      var index = sample(RandomInteger({n:tokens.length}))
      var replacement = transformFn(index)
      return _removeEmptyItems(replaceIndex(tokens, index, replacement)).join(' ')
  }  
}
var _deleteFn = function(s) {
  return ''
}
var deleteTransform = transformUtterance(_deleteFn)

var vacuousNoiseModel = function(utterance) {
  return utterance
}
var deleteNoiseModel = function(utterance) {
  return flip(THETA) ? deleteTransform(utterance) : utterance
}

var decode = cache(function(utteranceProduced, noiseModel) {
  Infer({
    model() {
      var utteranceIntended = utterancesPrior()
      var noisyUtterance = noiseModel(utteranceIntended, THETA)
      factor(utteranceProduced == noisyUtterance ? 0 : -Infinity)
      return utteranceIntended
    }
  })
})

var utteranceMeaning = function(utterance) {
  if (utterance == '') {
    console.log("Error on", utterance)
  }
  var lastCharIndex = utterance.length - 1
  if (utterance.includes('a')) {
    return 'a'
  } else if (utterance.includes('b')) {
    return 'b'
  } else if (utterance.includes('c')) {
    return 'c'
  } else if (utterance.includes('d')) {
    return 'd'
  } else {
    console.log("Error on", utterance)
  }
}

var meaning = function(utterance, world) {
  var worldDim = world['d']
  return utteranceMeaning(utterance) == worldDim
}


var cost = function(utterance) {
  return utterance.split(' ').length
}


var L0 = cache(function(producedUtterance, topic) {
  Infer({
    model() {
      var world = worldsGivenTopic(topic)
      var intendedUtterance = sample(decode(producedUtterance, deleteNoiseModel))
      var meaning = meaning(utteranceMeaning(intendedUtterance), world)
      factor(meaning ? 0 : -Infinity)
      return world
    }
  })
})


var S0Meaning = cache(function(topic) {
  Infer({
    model() {
      var estWorld = worldsGivenTopic(topic)
      var intendedUtterance = utterancesPrior()
      var producedUtterance = deleteNoiseModel(intendedUtterance)
      var L = L0(producedUtterance, topic)
      factor(ALPHA * (L.score(estWorld) - cost(intendedUtterance)/10))
      return estWorld
    }
  })
})

var S0Utterance = cache(function(topic) {
  Infer({
    model() {
      var estWorld = worldsGivenTopic(topic)
      var intendedUtterance = utterancesPrior()
      var producedUtterance = deleteNoiseModel(intendedUtterance)
      var L = L0(producedUtterance, topic)
      factor(ALPHA * (L.score(estWorld) - cost(intendedUtterance)/10))
      return intendedUtterance
    }
  })
})


var topicDistrGivenMeaningData = cache(function(observedData) {
  return Infer({method: 'enumerate'}, function() {
    var topicDistrName = sample(topicDistr)
    if (observedData != []) {
      var obsFn = function(datum){
        condition(datum['d'] == sample(S0Meaning(topicDistrName))['d'])
      }
      mapData({data: observedData}, obsFn)
    }
    return topicDistrName
  })
})

var topicDistrGivenUtteranceData = cache(function(observedData) {
  return Infer({method: 'enumerate'}, function() {
    var topicDistrName = sample(topicDistr)
    if (observedData != []) {
      var obsFn = function(datum){
        condition(datum == sample(S0Utterance(topicDistrName)))
      }
      mapData({data: observedData}, obsFn)
    }
    return topicDistrName
  })
})

var S1 = cache(function(world, topic) {
  Infer({
    model() {
      var intendedUtterance = utterancesPrior()
      var producedUtterance = deleteNoiseModel(intendedUtterance)
      var L = L0(producedUtterance, topic)
      factor(L.score(world))
      return intendedUtterance
    }
  })
})

var L1 = cache(function(producedUtterance, data) {
  Infer({
    model() {
      var estTopic = sample(topicDistrGivenUtteranceData(data))
      var estWorld = worldsGivenTopic(estTopic)
      var intendedUtterance = sample(decode(producedUtterance, deleteNoiseModel))
      var meaning = meaning(utteranceMeaning(intendedUtterance), estWorld)
      var S = S1(estWorld, estTopic)
      factor(LAMBDA * S.score(intendedUtterance))
      return {'world': estWorld, 'topic': estTopic}
    }
  })
})

var S2 = cache(function(world, topic, data) {
  Infer({
    model() {
      var intendedUtterance = utterancesPrior()
      var producedUtterance = deleteNoiseModel(intendedUtterance)
      var L = L1(producedUtterance, data)
      var res = {'world': world, 'topic': topic}
      factor(ALPHA * (L.score(res)-cost(intendedUtterance)/10))
      return {'utterance': intendedUtterance}
    }
  })
})


//
// Speaker utterance generation
//
var d = []
var speakerFn = function(T, d) {
  var world = worldsGivenTopic(T)
  return d.concat([sample(S2(world, T, d))['utterance']])
}

var speakerRecurse = function(T, d, n) {
  if (n == 0) {
    return d
  } else {
    return speakerRecurse(T, speakerFn(T, d), n-1)
  }
}


//
// Helpers
//
// Generate a sequence of numbers
var plusOne = function(n) {
  return n + 1
}

var range_ = function(start, end) {
  if (start == end) {
    return end
  } else {
    return [start].concat(range_(start+1, end))
  }
}
var range = function(start, end) {
  var res = range_(start, end)
  return res.slice(0, res.length-1)
}

// Listener probability assignment to topic "t_distr1"
var getDataSizes = function(n) {
  return range(0, n)
}

// List of slice sizes
var dataSizes = getDataSizes(nUtterances)
// A context slice is the size of the preceding words (e.g. 0-3, 0-10, 0-n)
var contextSlices = map(function(x) {return [0, x]}, dataSizes)


// getListenerInferences
// =====================
// Given a slice of speaker utterance data, return listeners state of belief
// about the intended topic distribution.
//
var getListenerInferences = function(utterances) {
  var res = map(function(x) {
    return {
      "T1": Math.exp(topicDistrGivenUtteranceData(utterances.slice(x[0], x[1])).score("T1")),
      "T2": Math.exp(topicDistrGivenUtteranceData(utterances.slice(x[0], x[1])).score("T2")),
      "T3": Math.exp(topicDistrGivenUtteranceData(utterances.slice(x[0], x[1])).score("T3"))}
  }, contextSlices)
  return res
}


// processUtterances
// =================
// Return a list of dictionaries containing speaker utterance data
// and corresponding state of listener inferences about topics
//
var processUtterances = function(speakerData, listenerInferences) {
  var dMap = map(function(x) {
    return {
      'utteranceNum': x, 
      'utterance': speakerData[x], 
      'T1': listenerInferences[x]['T1'],
      'T2': listenerInferences[x]['T2'],
      'T3': listenerInferences[x]['T3']}
  }, dataSizes)
  return dMap
}

// runFn
// =====
// Return data of speaker utterances / listener inferences
//
// Parameters
// ----------
// resultType : str
//   One of (contextAwareSpeaker, contextUnawareSpeaker, baseline)
//
// Returns
// --------
// List of dicts
//
var runFn = function(resultType, runReverse) {
  if (resultType == 'contextAwareS2') {
    var utteranceList = speakerRecurse(targetDistr, [], nUtterances)
    var listenerInferences = getListenerInferences(utteranceList)
    return processUtterances(utteranceList, listenerInferences)
  } 
  else if (resultType == 'contextUnawareS2') {
    var utteranceList = repeat(nUtterances, function(x) { sample(S2(targetDistr, []))['utterance']})
    var listenerInferences = getListenerInferences(utteranceList)
    return processUtterances(utteranceList, listenerInferences)
  } 
  else if (resultType == 'S1') {
    var utteranceList = repeat(nUtterances, (function(x) { return sample(S1(targetDistr))}))
    var listenerInferences = getListenerInferences(utteranceList)
    return processUtterances(utteranceList, listenerInferences)
  }
  else if (resultType == 'baseline') {
    var utteranceList = repeat(nUtterances, (function(x) {utterancesPrior()}))
    var listenerInferences = getListenerInferences(utteranceList)
    return processUtterances(utteranceList, listenerInferences)
  } 
  else {
    error('Bad resultType:', resultType)
  }
}
// runFn(resultType)
var test1 = ['X a', 'b', 'c']
var test2 = ['a', 'X b', 'c']
var test3 = ['a', 'b', 'X c']
var test4 = ['a', 'b', 'c']
viz.table(topicDistrGivenUtteranceData(test1))
viz.table(topicDistrGivenUtteranceData(test2))
viz.table(topicDistrGivenUtteranceData(test3))
viz.table(topicDistrGivenUtteranceData(test4))

// var T1 = Categorical({vs:worlds, ps:[1, 0.5, 0.2]})
// var T2 = Categorical({vs:worlds, ps:[0.2, 1, 0.5]}) 
// var T3 = Categorical({vs:worlds, ps:[0.5, 0.2, 1]})
runFn(resultType)