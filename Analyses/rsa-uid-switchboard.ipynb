{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSA-UID-switchbaord\n",
    "\n",
    "Investigating (possibly) meaningful disfluencies in switchboard\n",
    "\n",
    "### Objectives\n",
    "1. analysis: do disfluencies appear before high-surprisal items?\n",
    "2. listener exp: are listeners inclined to choose high-surprisal items as good candidates when they see 'uh'\n",
    "\n",
    "### Resources\n",
    "* Switchboard http://www.nltk.org/_modules/nltk/corpus/reader/switchboard.html\n",
    "* typing animation https://codepen.io/rusjames/pen/uAFhE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import switchboard as sb\n",
    "import numpy as np\n",
    "import pdb\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example discourse..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "discourses = [d for d in sb.discourses()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<A.1: 'Uh , do you have a pet Randy ?'>,\n",
       " <B.2: 'Uh , yeah , currently we have a poodle .'>,\n",
       " <A.3: 'A poodle , miniature or , uh , full size ?'>,\n",
       " <B.4: \"Yeah , uh , it 's , uh miniature .\">,\n",
       " <A.5: 'Uh-huh .'>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discourses[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_start(s):\n",
    "    start_patt = r\"^<[AB]\\.[0-9]{1,}:\\s[\\'\\\"]\"\n",
    "    replace_patt = ''\n",
    "    s = re.sub(start_patt, replace_patt, s)\n",
    "    return s\n",
    "\n",
    "def replace_end(s):\n",
    "    end_patt = r\"\\s?\\'?>\"\n",
    "    replace_patt = ''\n",
    "    s = re.sub(end_patt, replace_patt, s)\n",
    "    return s\n",
    "\n",
    "def remove_punct(s):\n",
    "    s = re.sub('[,\\.\\?:\\{\\}\\[\\]]', '', s)\n",
    "    return s\n",
    "\n",
    "def turn2Str(t):\n",
    "    unicodeStr = t.unicode_repr()\n",
    "    s = preprocess_turn_str(unicodeStr)\n",
    "    return s\n",
    "\n",
    "def preprocess_turn_str(s):\n",
    "    s = replace_start(s)\n",
    "    s = replace_end(s)\n",
    "    s = remove_punct(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub('\\s{2,}', ' ', s)\n",
    "    s = re.sub(r'\"$', '', s)\n",
    "    s = s.rstrip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<A.3: 'A poodle , miniature or , uh , full size ?'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a poodle miniature or uh full size'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exTurn = discourses[0][2]\n",
    "print(str(exTurn))\n",
    "turn2Str(exTurn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allStrs = [turn2Str(t) for d in discourses for t in d]\n",
    "allStrsStartEnd = ['^ ' + s + ' $' for s in allStrs]\n",
    "orderedTokens = ' '.join(allStrsStartEnd).split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self, n=2):\n",
    "        self.n = n\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "    def ingest(self, orderedTokens):\n",
    "        self.vocab_size = len(orderedTokens)\n",
    "        d = defaultdict(Counter)\n",
    "        for i, word in enumerate(orderedTokens):\n",
    "            if i == len(orderedTokens)-self.n:\n",
    "                break\n",
    "            currTup = tuple(orderedTokens[i:i+self.n-1])\n",
    "            d[currTup][orderedTokens[i+self.n-1]] += 1\n",
    "        self.lm = d\n",
    "        \n",
    "    def get_lm(self):\n",
    "        return self.lm\n",
    "    \n",
    "    def get_candidates(self, token):\n",
    "        return self.lm[token]\n",
    "    \n",
    "    def get_lm_prob(self, tup):\n",
    "        context = tup[0:self.n-1]\n",
    "        target = tup[self.n-1]\n",
    "        num = self.lm[context][target]\n",
    "        denom = np.sum([d[1] for d in self.lm[context].items()])\n",
    "        return num / denom\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ng = Ngram(n=3)\n",
    "exCorpus = ['apple', 'on', 'the', 'apple', 'on', 'a', 'range', 'apple']\n",
    "ng.ingest(exCorpus)\n",
    "ng.get_lm_prob(('apple', 'on', 'apple'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build switchboard ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ng = Ngram(2)\n",
    "ng.ingest(orderedTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - brainstorm\n",
    "\n",
    "* Task: sentence completion - particiants see a sentence fragment and have to fill in the missing item from a choice of five candidates. \n",
    "    + include the \"true\" candidate?\n",
    "    + include a spectrum of high->low suprisal items\n",
    "* What if we had people incrementally build sentences from the LM candidates?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sent = random.choice(allStrs).split(' ')\n",
    "# index = random.randint(0, len(sent))\n",
    "# sent[:index]\n",
    "exSent = 'yeah i have a uh well a mutt myself i call it a uh uh chowperd'\n",
    "exSentSplit = exSent.split(' ')\n",
    "index = random.randint(1, len(exSentSplit)-1)\n",
    "fragment = exSentSplit[:index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "lastToken = fragment[-1]\n",
    "print(lastToken)\n",
    "trueCandidate = exSentSplit[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "lm_candidates = list(ng.get_candidates().keys())\n",
    "print(lm_candidates)\n",
    "candidates = set(random.sample(lm_candidates, min(5, len(lm_candidates)))) | {trueCandidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full utterace:\tyeah i have a uh well a mutt myself i call it a uh uh chowperd\n",
      "Fragment:\tyeah i have a uh well a mutt myself i call it a uh uh\n",
      "Last token:\tuh\n",
      "Candidates:\t{'chowperd'}\n",
      "True candidate:\tchowperd\n"
     ]
    }
   ],
   "source": [
    "print(\"Full utterace:\\t{}\".format(exSent))\n",
    "print('Fragment:\\t{}'.format(' '.join(fragment)))\n",
    "print('Last token:\\t{}'.format(lastToken))\n",
    "print('Candidates:\\t{}'.format(candidates))\n",
    "print('True candidate:\\t{}'.format(trueCandidate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment - just \"uh's\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ well this was not a learning thing by any means it was just a uh $'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uhStrs = [s for s in allStrsStartEnd if 'uh' in s.split()]\n",
    "uhStrs_sample = random.sample(uhStrs, 10)\n",
    "uhStrs_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_match_indices(x, s):\n",
    "    return [i for i, w in enumerate(s.split(' ')) if w == x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = get_match_indices('uh', uhStrs_sample[0])\n",
    "index = indices[0]; index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ well this was not a learning thing by any means it was just a uh'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(uhStrs_sample[0].split(' ')[0:index+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class categoricalDistr:\n",
    "    def __init__(self, vs, ps):\n",
    "        self.vs = vs\n",
    "        self.ps = ps\n",
    "        self.normed_ps = self.normalize(self.ps)\n",
    "        self.distr = dict(zip(self.vs, self.normed_ps))\n",
    "        \n",
    "    def sample(self, k=5):\n",
    "        vals = np.random.choice(a=self.vs, size=(k), p=self.normed_ps)\n",
    "        d = {}\n",
    "        for v in vals:\n",
    "            d[v] = self.distr[v]\n",
    "        return d\n",
    "\n",
    "    def normalize(self, x):\n",
    "        return x / np.sum(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vs = list(ng.get_candidates('uh').keys())\n",
    "ps = list(ng.get_candidates('uh').values())\n",
    "C = categoricalDistr(vs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$': 0.056916996047430828,\n",
       " 'even': 0.0015810276679841897,\n",
       " 'have': 0.0023715415019762848,\n",
       " 'i': 0.11304347826086956,\n",
       " 'uh': 0.039525691699604744}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Levy & Jaeger (2007) that-omission analysis (section 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Objective**:\n",
    "What is the relationship between `uh/um` presence and the informativity of the following text?\n",
    "* H_0: there is no relationship\n",
    "* H_1: presence of `uh/um` indicates the following content is high-surprisal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "ng = Ngram(n)\n",
    "ng.ingest(orderedTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^ uh do you have a pet randy $'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uhStrs = [s for s in allStrsStartEnd if 'uh' in s.split()]\n",
    "uhStrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKtuple(s, k=2):\n",
    "    s_split = s.split(' ')\n",
    "    d = []\n",
    "    for i, token in enumerate(s_split):\n",
    "        if i == len(s_split)-k:\n",
    "            break\n",
    "        curr_tup = tuple(s_split[i:i+k])\n",
    "        d.append(curr_tup)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('^', 'uh', 'do'),\n",
       " ('uh', 'do', 'you'),\n",
       " ('do', 'you', 'have'),\n",
       " ('you', 'have', 'a'),\n",
       " ('have', 'a', 'pet'),\n",
       " ('a', 'pet', 'randy')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getKtuple(uhStrs[0], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('^', 'uh', 'do'), 0.007042253521126761\n",
      "('uh', 'do', 'you'), 0.5\n",
      "('do', 'you', 'have'), 0.2112676056338028\n",
      "('you', 'have', 'a'), 0.24285714285714285\n",
      "('have', 'a', 'pet'), 0.008771929824561403\n",
      "('a', 'pet', 'randy'), 1.0\n"
     ]
    }
   ],
   "source": [
    "for tup in getKtuple(uhStrs[0], n):\n",
    "    print('{}, {}'.format(tup, ng.get_lm_prob(tup)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "* Get all instances of find all context / targets (w/ and w/o disfluencies and get surprisal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
