---
title: "Rsa-uid-discourse-20180212"
author: "Ben"
date: "2/12/2018"
output: html_document
---

Imagine that natural language is a succession of reference games. Success in each reference game is blind to the greater context. Ane yet

```{r libraries, message=FALSE, warning=FALSE}
rm(list = ls())  # clear workspace

library(doParallel)
library(dplyr)
library(entropy)
library(ggplot2)
library(ggthemes)
library(tidyr)
library(zoo)


source("run-helpers.R")
```

```{r}
fPath <- '/Users/benpeloquin/Desktop/Projects/rsa_uid/Models/discourse/rsa-uid-discourse-20180215.wppl'
m <- getModelFile(fPath)
```

```{r runFn}
NUM_UTTERANCES <- 120
run <- createRunFn(m)
runFn <- function(i, targetDistr, nUtterances, resultType, alpha, reverseResults) {
  dTemp <- data.frame(targetDistr=targetDistr, nUtterances=nUtterances, resultType=resultType, alpha=alpha, reverseResults=reverseResults)
  df <- run(dTemp) %>%
    mutate(runNum=i, targetDistr=targetDistr, alpha=alpha, resultType=resultType, reverseResults=reverseResults)
  df
}
```

```{r singleRun}
ALPHA <- 10
TOPIC <- 't_distr1'
df_baseline <-        runFn(1, TOPIC, NUM_UTTERANCES, 'baseline', ALPHA, 'false')
df_contextUnwareS2 <- runFn(1, TOPIC, NUM_UTTERANCES, 'contextUnawareS2', ALPHA, 'false')
df_contextAwareS2 <-  runFn(1, TOPIC, NUM_UTTERANCES, 'contextAwareS2', ALPHA, 'false')
df_S1 <-              runFn(1, TOPIC, NUM_UTTERANCES, 'S1', ALPHA, 'false')
# df_baselineReverse <- runFn(1, NUM_UTTERANCES, 'baseline', ALPHA, 'true')
# df_contextUnwareS2Reverse <- runFn(1, NUM_UTTERANCES, 'contextUnawareS2', ALPHA, 'true')
# df_contextAwareS2Reverse <- runFn(1, NUM_UTTERANCES, 'contextAwareS2', ALPHA, 'true')
# df_S1Reverse <- runFn(1, NUM_UTTERANCES, 'S1', ALPHA, 'true')

df_agg <- rbind(df_baseline, df_contextUnwareS2, df_contextAwareS2, df_S1)
```

### Listener convergence as a function of speaker...
```{r listenerInferencePlot}
df_agg %>% 
  gather(topic, listenerTopicPosterior, c(t_distr1, t_distr2, t_distr3, t_distr4)) %>%
  mutate(topic=ifelse(topic==TOPIC, 'target', topic)) %>%
  ggplot(aes(x=utteranceNum, y=listenerTopicPosterior, col=topic)) +
    geom_line() +
    # geom_vline(data=df_agg %>% select(resultType, pos) %>% unique, aes(xintercept=pos), lty=2)+
    theme_few() +
    facet_grid(resultType~reverseResults)
```

### Speaker utterances over time
```{r inspectingUtterancePlot}
df_agg %>%
  mutate(utteranceVal=ifelse(utterance=='a', 1, ifelse(utterance=='b', 1, 1))) %>%
  ggplot(aes(x=utteranceNum, y=utteranceVal, col=utterance), alpha=0.6) +
    # geom_vline(data=df_agg %>% select(resultType, pos) %>% unique, aes(xintercept=pos), lty=2) +
    geom_text(aes(x=utteranceNum, label=utterance, y=utteranceVal, col=utterance)) +
    ylim(0.75, 1.25) +
    theme_few() +
    facet_grid(resultType~reverseResults)
```

## Close look -- good for inspecting small scenerios
```{r}
df_agg %>% 
  mutate(utteranceVal=0.5) %>%
  filter(utteranceNum < 25) %>%
  gather(topic, listenerTopicPosterior, c(t_distr1, t_distr2, t_distr3)) %>%
  ggplot(aes(x=utteranceNum, y=listenerTopicPosterior, col=topic)) +
    geom_text(aes(x=utteranceNum, label=utterance, y=utteranceVal, col=utterance)) +
    geom_line(alpha=0.3, size=1) +
    # geom_vline(data=df_agg %>% select(resultType, pos) %>% unique, aes(xintercept=pos), lty=2)+
    theme_few() +
    facet_grid(resultType~reverseResults)
```


How do speaker choices change as a function of listener beliefs?
```{r speakerChoicePreProcess}
binSize <- 12
breaks <- seq(0, NUM_UTTERANCES, by=binSize)

df_agg <- df_agg %>%
  mutate(bin=cut(utteranceNum, breaks=breaks, right=TRUE, include.lowest=TRUE))
bin_levels <- levels(df_agg$bin)
df_agg$binVal <- match(df_agg$bin, bin_levels)
```

# fill missing values
```{r preprocessFillMissingValues}
# Get utterance counts
df_utteranceTotals <- df_agg %>%
  group_by(resultType, binVal, utterance) %>%
  summarise(n=n()) %>%
  ungroup

# Create a data.frame to merge for place-hodlders
d_fill <- data.frame(expand.grid(resultType=unique(df_utteranceTotals$resultType), 
                                 utterance=unique(df_utteranceTotals$utterance), 
                                 binVal=unique(df_utteranceTotals$binVal))) %>%
  mutate(utterance=as.character(utterance),
         n=0)

# Run merge (see https://stackoverflow.com/questions/7735647/replacing-nas-with-latest-non-na-value)
df_agg_filled <- merge(df_utteranceTotals, d_fill, by=c('resultType', 'binVal', 'utterance'), all=TRUE) %>%
  select(resultType, binVal, utterance, resultType, n.x) %>%
  mutate(n.x=ifelse(is.na(n.x), 0, n.x),
         resultType=na.locf(resultType, fromLast=TRUE)) %>%
  rename(n=n.x)
```


```{r utteranceProportionsPreProcess}
# Get group totals, these are uneven, depending on cuts
df_groupTotals <- df_agg_filled %>%
  group_by(resultType, binVal) %>%
  summarize(groupTotal=sum(n))


# Create utterance proportions
df_filled_uttProbs <- left_join(df_agg_filled, df_groupTotals, by=c('resultType', 'binVal')) %>%
  mutate(prop=n/groupTotal)
# Check props are calculated correctly
# all(
#   as.list(df_filled_uttProbs %>%
#             group_by(resultType, binVal) %>%
#             summarise(s=sum(prop)) %>%
#             ungroup %>%
#             select(s))$s)
```


```{r utteranceProportionsPlot}
df_filled_uttProbs %>%
  ggplot(aes(x=binVal, y=prop, fill=utterance)) +
    # geom_vline(data=df_agg %>% select(resultType, posBin) %>% unique, aes(xintercept=posBin), lty=2) +
    geom_bar(stat='identity', alpha=0.5) +
    facet_grid(~resultType) +
    theme_few() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r entropy1}
breaks <- seq(0, NUM_UTTERANCES, by=binSize)
# Replace with entropy.empirical()
# calcEntropy <- function(a) {
#   -1 * sum(sapply(a, function(x) {
#     log2(x) * x
#   }))
# }

df_filled_uttProbs %>%
  group_by(resultType, binVal) %>%
  summarise(entropy=entropy.empirical(n, unit=c('log2'))) %>%
  ggplot(aes(x=binVal, y=entropy)) +
    geom_line(group=1) +
    theme_few() +
    facet_grid(~resultType)
```




##H(Y) = $H(X|L) - I(X|L, C)$
```{r}
norm_ <- function(x) {
  x / sum(x)
}

end <- length(breaks) -1
endPoints <- seq(1, end)
sapply(endPoints, function(x) {
    empCnts <- as.list(
      df_filled_uttProbs %>%
        # Note (BP): Think about the slices here.
        filter(resultType=='contextAwareS2', binVal %in% seq(x, x, by=1)) %>%
        group_by(utterance) %>%
        summarise(cnt=sum(n)) %>%
        select(cnt))$cnt
    ent <- entropy::entropy(norm_(empCnts), unit=c('log2'))
    ent
  })
```


BP here 20180218
```{r}
# Helpers
norm_ <- function(x) {
  x / sum(x)
}
getMI <- function(x, y) {
  entropy::entropy(norm_(x), unit=c('log2')) + entropy::entropy(norm_(y), unit=c('log2')) -
      entropy::entropy(c(norm_(x), norm_(y)), unit=c('log2'))
}

end <- length(breaks)-1
endPoints <- seq(1, end)
getEntropyData <- function(df, model, endPoints, actualProps) {
  
  MIs <- sapply(endPoints, function(x) {
    uncondEntropy <- as.list(
      df %>%
        # Note (BP): Think about the slices here.
        filter(resultType==model, binVal %in% seq(x, x, by=1)) %>%
        select(n))$n
    condEntropy <- as.list(
      df %>%
        # Note (BP): Think about the slices here.
        filter(resultType==model, binVal %in% seq(1, x, by=1)) %>%
        group_by(utterance) %>%
        summarise(cnt=sum(n)) %>%
        select(cnt))$cnt
    # entropy <- entropy(uncondEntropy, condEntropy, unit=c('log2'))
    MI <- getMI(actualProps, uncondEntropy)
    MI
    })
  
  uncondEntropies <- sapply(endPoints, function(x) {
    empCnts <- as.list(
      df %>%
        # Note (BP): Think about the slices here.
        filter(resultType==model, binVal %in% seq(x, x, by=1)) %>%
        group_by(utterance) %>%
        summarise(cnt=sum(n)) %>%
        select(cnt))$cnt
    ent <- entropy::entropy(norm_(empCnts), unit=c('log2'))
    ent
  })

  res_df <- data.frame(endPoint=endPoints, 
                       uncondEntropy=uncondEntropies,
                       MI=MIs) %>%
    mutate(hY=uncondEntropy-MI)
  res_df
}

topicProps <- c(0.6, 0.25, 0.15, 0.15, 0.1, 0.1)
# actualCnts <- c(4, 4, 10)
# infotheo::mutinformation(round(norm_(actualCnts)*10), topicProps*10)
# jointEntropy(topicProps, actualCnts)
# jointEntropy(actualCnts, topicProps)

# infotheo::mutinformation(topicProps*10, topicProps*1000)
resultTypes <- c('contextAwareS2', 'contextUnawareS2', 'baseline', 'S1')
df_entropies <- data.frame()
for (resultType_ in resultTypes) {
  currDf <- getEntropyData(df_filled_uttProbs, resultType_, endPoints, topicProps) %>%
    mutate(resultType=resultType_)
  df_entropies <- rbind(df_entropies, currDf)
}
```

```{r}
df_entropies %>% 
  gather(type, val, c(uncondEntropy, MI, hY)) %>%
  ggplot(aes(x=endPoint, y=val, col=type)) +
    geom_point(alpha=0.7) +
    geom_smooth(method='lm') +
    theme_few() +
    facet_grid(~resultType)
```


# `Context aware speaker` parallelize runs

```{r runParContextAware}
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores, type='FORK')
registerDoParallel(cl)

ptm <- proc.time()
nSims <- 100
sims_contextAwareSpeaker <- foreach(i=seq(1, nSims), .packages=c('dplyr', 'rwebppl'), .combine=rbind) %dopar% 
  runFn(i, TOPIC, NUM_UTTERANCES, 'contextAwareS2', ALPHA, 'false')
stopCluster(cl)
etm <- proc.time() - ptm
cat("runtime: ", etm[3] / 60)
```


```{r entropyPreprocessContextAware}
df_simscontextAwareSpeaker <- sims_contextAwareSpeaker %>%
  mutate(utteranceNum=(utteranceNum)) %>%
  mutate(bin=cut(utteranceNum, breaks=breaks, right=TRUE, include.lowest=TRUE))
bin_levels <- levels(df_simscontextAwareSpeaker$bin)
df_simscontextAwareSpeaker$binVal <- match(df_simscontextAwareSpeaker$bin, bin_levels)

# Get group totals, these are uneven, depending on cuts
# df_simscontextAwareSpeaker_groupTotals <- df_simscontextAwareSpeaker %>%
#   group_by(runNum, resultType, binVal) %>%
#   summarize(groupTotal=n())

# Get utterance counts
df_simscontextAwareSpeaker_utteranceTotals <- df_simscontextAwareSpeaker %>%
  group_by(runNum, resultType, binVal, utterance) %>%
  summarise(n=n()) %>%
  ungroup
```

```{r entropyPlotContextAware}
# Plot with individual points
# df_simscontextAwareSpeaker_utteranceTotals %>%
#   group_by(runNum, binVal) %>%
#   summarise(entropy=entropy::entropy(norm_(n), unit=c('log2'))) %>%
#   ggplot(aes(x=binVal, y=entropy)) +
#     geom_jitter(alpha=0.2, width=0.05, height=0) +
#     geom_smooth(method='lm', col='green') +
#     geom_smooth(method='loess') +
#     theme_few()

df_simscontextAwareSpeaker_utteranceTotals %>%
  group_by(runNum, binVal) %>%
  summarise(entropy=entropy::entropy(norm_(n), unit=c('log2'))) %>%
  ggplot(aes(x=binVal, y=entropy)) +
    geom_smooth(method='loess') +
    theme_few()

# I think this is the wrong plot -- we're as each of the bins are correlated
# df_simscontextAwareSpeaker_uttProbs %>%
#   group_by(runNum, binVal) %>%
#   summarise(entropy=calcEntropy(prop)) %>%
#   group_by(binVal) %>%
#   summarise(avg=mean(entropy),
#             std=sd(entropy),
#             ci_low=avg - 2*std,
#             ci_high=avg + 2*std) %>%
#   ggplot(aes(x=binVal, y=avg)) +
#     geom_point() +
#     geom_errorbar(aes(ymin=ci_low, ymax=ci_high), width=0) +
#     ylim(0, 2.5) +
#     theme_few()
```


## Boostrapped H(Y) = H(X|L) - I(X,C|L)
```{r}
dTemp <- df_simscontextAwareSpeaker_utteranceTotals %>%
  filter(resultType == 'contextAwareS2')

dTempEntropies <- dTemp %>%
  group_by(runNum) %>%
  do(getEntropyData(., 'contextAwareS2', endPoints, topicProps))

dTempEntropies %>%
  gather(type,val, c(uncondEntropy, MI, hY)) %>%
  ggplot(aes(x=endPoint, y=val, col=type)) +
    # geom_jitter(alpha=0.2, width=0.25) +
    geom_smooth(method='loess') +
    theme_few()
  
```


```{r lm-model}
d <- df_simscontextAwareSpeaker_utteranceTotals %>%
  group_by(runNum, binVal) %>%
  summarise(entropy=entropy.empirical(n, unit=c('log2')))

summary(lm(entropy~poly(binVal, 2), data=d))
```

# `Context unaware speaker` parallelize runs
```{r runParContexUnaware}
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores, type='FORK')
registerDoParallel(cl)

ptm <- proc.time()
nSims <- 100
sims_contextUnawareSpeaker <- foreach(i=seq(1, nSims), .packages=c('dplyr', 'rwebppl'), .combine=rbind) %dopar% 
  runFn(i, TOPIC, NUM_UTTERANCES, 'contextUnawareS2',  ALPHA, 'false')
stopCluster(cl)
etm <- proc.time() - ptm
cat("runtime: ", etm[3] / 60)
```

```{r entropyPreprocessContextUnaware}
df_sims_unawareSpeaker <- sims_contextUnawareSpeaker %>%
  mutate(utteranceNum=(utteranceNum)) %>%
  mutate(bin=cut(utteranceNum, breaks=breaks, right=TRUE, include.lowest=TRUE))
bin_levels <- levels(df_sims_unawareSpeaker$bin)
df_sims_unawareSpeaker$binVal <- match(df_sims_unawareSpeaker$bin, bin_levels)

# Get group totals, these are uneven, depending on cuts
df_sims_unawareSpeaker_groupTotals <- df_sims_unawareSpeaker %>%
  group_by(runNum, resultType, binVal) %>%
  summarize(groupTotal=n())

# Get utterance counts
df_sims_unawareSpeaker_utteranceTotals <- df_sims_unawareSpeaker %>%
  group_by(runNum, resultType, binVal, utterance) %>%
  summarise(n=n()) %>%
  ungroup

# Create utterance proportions
df_sims_unawareSpeaker_uttProbs <- left_join(df_sims_unawareSpeaker_utteranceTotals, 
                                             df_sims_unawareSpeaker_groupTotals, 
                                             by=c('runNum', 'resultType', 'binVal')) %>%
  mutate(prop=n/groupTotal)
```

```{r entropyPlotContextUnaware}
df_sims_unawareSpeaker_uttProbs %>%
  group_by(runNum, binVal) %>%
  summarise(entropy=calcEntropy(prop)) %>%
  ggplot(aes(x=binVal, y=entropy)) +
    geom_jitter(alpha=0.2, width=0.05, height=0) +
    geom_smooth(method='lm', col='green') +
    geom_smooth(method='loess') +
    theme_few()
```

```{r runParS1}
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores, type='FORK')
registerDoParallel(cl)

ptm <- proc.time()
nSims <- 100
sims_S1 <- foreach(i=seq(1, nSims), .packages=c('dplyr', 'rwebppl'), .combine=rbind) %dopar% runFn(i, TOPIC, NUM_UTTERANCES, 'S1',  ALPHA, 'false')
stopCluster(cl)
etm <- proc.time() - ptm
cat("runtime: ", etm[3] / 60)
```

```{r entropyPreprocessBaseline}
df_sims_S1 <- sims_S1 %>%
  mutate(utteranceNum=(utteranceNum)) %>%
  mutate(bin=cut(utteranceNum, breaks=breaks, right=TRUE, include.lowest=TRUE))
bin_levels <- levels(df_sims_S1$bin)
df_sims_S1$binVal <- match(df_sims_S1$bin, bin_levels)

# Get group totals, these are uneven, depending on cuts
df_S1_groupTotals <- df_sims_S1 %>%
  group_by(runNum, resultType, binVal) %>%
  summarize(groupTotal=n())

# Get utterance counts
df_S1_utteranceTotals <- df_sims_S1 %>%
  group_by(runNum, resultType, binVal, utterance) %>%
  summarise(n=n()) %>%
  ungroup

# Create utterance proportions
df_S1_uttProbs <- left_join(df_S1_utteranceTotals, df_S1_groupTotals, by=c('runNum', 'resultType', 'binVal')) %>%
  mutate(prop=n/groupTotal)
```

```{r entropyPlotS1}
df_S1_uttProbs %>%
  group_by(runNum, binVal) %>%
  summarise(entropy=calcEntropy(prop)) %>%
  ggplot(aes(x=binVal, y=entropy)) +
    geom_smooth(method='loess') +
    theme_few()
```



**Caution** this plot is slow running...
```{r convergencePar, val=FALSE}
# sims %>% 
#   mutate(utteranceNum=utteranceNum+1) %>%
#   gather(topic, listenerTopicPosterior, c(t_distr1, t_distr2, t_distr3)) %>%
#   group_by(utteranceNum, topic) %>%
#   summarise(avgListenerPost=mean(listenerTopicPosterior),
#             sdListenerPost=sd(listenerTopicPosterior),
#             lowerListenerPost=avgListenerPost-2*sdListenerPost,
#             upperListenerPost=avgListenerPost+2*sdListenerPost) %>%
#   ggplot(aes(x=utteranceNum, y=avgListenerPost, col=topic)) +
#     geom_line() +
#     theme_few()

# CAUTION (BP): Slow running!
sims_contextAwareSpeaker %>% 
  mutate(utteranceNum=utteranceNum+1) %>%
  gather(topic, listenerTopicPosterior, c(t_distr1, t_distr2, t_distr3)) %>%
  ggplot(aes(x=utteranceNum, y=listenerTopicPosterior, col=topic)) +
    stat_smooth(method='loess', se=TRUE) +
    theme_few()
```

```{r entropyParPlot}
breaks <- seq(0, NUM_UTTERANCES, by=10)

calcEntropy <- function(a) {
  -1 * mean(sapply(a, function(x) {
    log2(x) * x
  }))
}

# sims %>%
#   mutate(utteranceNum=(utteranceNum+1)) %>%
#   mutate(bin=cut(utteranceNum, breaks=breaks)) %>%
#   group_by(runNum, bin, utterance) %>%
#   summarise(n=n(),
#             prob=n/10) %>%
#   ungroup %>%
#   group_by(runNum, bin) %>%
#   summarise(entropy=calcEntropy(prob))
#   ungroup %>%
#   group_by(bin) %>%
#   summarise(avgEntropy=mean(entropy),
#             sdEntropy=sd(entropy),
#             lowerEntropy=avgEntropy - 2*sdEntropy,
#             upperEntropy=avgEntropy + 2*sdEntropy)
  



df_sims <- sims %>%
  mutate(utteranceNum=(utteranceNum+1)) %>%
  mutate(bin=cut(utteranceNum, breaks=breaks))
bin_levels <- levels(df_sims$bin)
df_sims$binVal <- match(df_sims$bin, bin_levels)

df_sims %>%
  group_by(runNum, binVal, utterance) %>%
  summarise(n=n(),
            prob=n/10) %>%
  ungroup %>%
  group_by(runNum, binVal) %>%
  summarise(entropy=calcEntropy(prob)) %>%
  ggplot(aes(x=binVal, y=entropy)) +
    geom_smooth(method='loess') +
    theme_few()
  
```


RSA and optimal orderings
```{r}
t1 <- c(0.6, 0.25, 0.15)
t2 <- c(0.6, 0.15, 0.25)
t3 <- c(0.25, 0.6, 0.15)
t1 <- c(0.4, 0.4, 0.2)
t2 <- c(0.4, 0.2, 0.4)
t3 <- c(0.2, 0.4, 0.4)
m <- matrix(c(t1, t2, t3), nrow=3)

reason <- function(m, n=1) {
  new_m <- m
  if (n==0) {return(new_m)}
  for (i in 1:n) {c
    new_m <- sweep(t(new_m), 2, colSums(t(new_m)), FUN="/")
  }
  new_m
}

iters <- seq(1, 10)
res <- sapply(iters, function(x) {reason(m, n=x)[1,1]})
data.frame(iters=iters, res=res) %>%
  ggplot(aes(x=iters, y=res)) +
    geom_line()
```

