---
title: "Uid-variance-analysis"
author: "Ben"
date: "4/10/2018"
output: html_document
---

```{r libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(knitr)
library(lme4)
library(lmerTest)
library(tidytext)

source("summary-spring-2018-helpers.R")
```

UID predicts rational speakers should attempt to produce utterances that are approximately uniform in their information rate. One way of doing this is by providing cues before high-surprisal content. We examine the degree to which RSA models display provide cues before high-suprisal content in [`summary-spring-2018.Rmd`](https://htmlpreview.github.io/?https://github.com/benpeloquin7/rsa-uid/blob/master/project/summary-spring-2018.html). Relatdely, we can examine another measure of smoothness -- looking at the degree to which ngram units vary in their surprisal over a discourse. Smaller variance means "smoother" production. We pursue this idea in the current notebook.

Simulate `n` corpora from `noisy-RSA` model of length `n_tokens`. Given a particular string of tokens we can randomly shuffle the UID cues. If part of what RSA is doing is optimizing for smoothness we should see lower variance in ngram surprisals among the `RSA ordering` vs a `random ordering` of cues. For each corpora $i$ of the $n$ simulations we randomly shuffle the UID cues $n_comparison$ times, comparing the average ngram surprisal variance across these simulations.

```{r model-files, warning=FALSE, message=FALSE, echo=FALSE}
plot3NoiseModelFile <- "/Users/benpeloquin/Desktop/Projects/rsa_uid/Models/fyp-presentation/noisy-rsa-corpus-model.wppl"
```

```{r helpers, warning=FALSE, message=FALSE, echo=FALSE}

# countCharOccurrences
# ====================
# 
# Parameters
# ----------
# char : char
#    Character to count occurences of in string.
# s : str
#   String to search in.
# 
#  Returns
#  -------
#  int
#     Number of occurances of 'char' in s
#
countCharOccurrences <- function(char, s) {
  s2 <- gsub(char,"",s)
  nchar(s) - nchar(s2)
}

# buildBigramsDf
# ==============
# 
# Parameters
# ----------
# utteranceStrNoSpaces: string
#   Character string with spaces removed.
# 
# Returns
# -------
# data.frame
#   Data frame with bigram counts (modified from ngram package)
# 
buildBigramsDf <- function(utteranceStrNoSpaces) {
  ng <- ngram::ngram(utteranceStrNoSpaces, n=2, sep='')
  dfNgram <- data.frame(ngram::get.phrasetable(ng))
  dfNgram$first <- sapply(dfNgram$ngrams, function(x) {strsplit(x, '')}[[1]][1])
  dfNgram$second <- sapply(dfNgram$ngrams, function(x) {strsplit(x, '')}[[1]][3])
  dfNgram <- dfNgram %>%
    mutate(first=ifelse(first==' ', '*', first),
           second=ifelse(second==' ', '*', second)) %>%
    # I think this should be first instead of second
    # we want P(a|x) not P(x|a) (as in the UID likelihood)
    group_by(first) %>%
    mutate(firstTotalProp=sum(prop),
           bigramProb=prop/firstTotalProp) %>%
    rename(bigrams=ngrams)  %>%
    ungroup %>%
    select(bigrams, freq, prop, firstTotalProp, bigramProb) %>%
    mutate(runNum=0)
  dfNgram
}

# runRandomXsFn
# =============
# 
# Parameters
# ----------
# char_vector_no_xs: str
#   Character vector (assume UID cues have been removed).
# numXs: int
#   Number of cues in string.
# 
# Returns
# -------
# str
#   Character vector with numXs inserted.
# 
runRandomXsFn <- function(char_vector_no_xs, numXs) {
  X_positions <- sample(nchar(char_vector_no_xs), numXs)
  newVector <- strsplit(char_vector_no_xs, split="")[[1]]
  for (i in X_positions) {
    newVector[i] <- paste0("X", newVector[i])
  }
  newVector <- paste(newVector, collapse="")
  newVector
}

# buildRandomLMs
# ==============
# 
# Parameters
# ----------
# df: data.frame
#   Data frame from previous run with LM counts
# allUtterancesCuesRemoved: str
#   Character vector with cues (e.g. 'X's) removed
# nComparison: int
#   Number of random comparisons to make.
# expNum: int
#   Meta data for simulation tracking
# 
# Returns
# -------
# data.frame
#   New data frame of original df ('df') and nComparisos LMs
# 
buildRandomLMs <- function(df, allUtterancesCuesRemoved, nCues, nComparisons, expNum) {
  df_agg <- df
  for (i in seq(1, nComparisons)) {
    randomSample <- runRandomXsFn(allUtterancesCuesRemoved, nCues)
    df_random <- buildBigramsDf(randomSample) %>%
      mutate(runNum=i,
             expNum=expNum,
             type="random")
    df_agg <- rbind(df_agg, df_random)
  }
  df_agg <- df_agg %>%
    mutate(bigrams=gsub(" $", "", bigrams))
  df_agg
}

# mergeLMsUtterances
# ==================
# 
# Parameters
# ----------
# df_run: data.frame
#   (Original) Data frame from call to runExp function.
# df_agg: data.frame
#   Data frame from call to buildRandomLms()
# 
# Returns
# -------
# data.frame
#   Merged data.frame where each row is an distinct bigram
#   with informativity information attached.
# 
mergeLMsUtterances <- function(df_run, df_agg, expNum) {
    df_bigrams <- df_run %>%
      select(currUtterance) %>%
      unnest_tokens("text", currUtterance, token="ngrams", n=2, to_lower=FALSE) %>%
      rename(bigrams=text)
  df_bigrams$i  <- seq(1, nrow(df_bigrams))
  # NOTE (BP): Assumes presence of `bigrams`, `bigramProb`, and `runNum` in df_agg
  left_join(df_bigrams, df_agg %>% select(bigrams, bigramProb, runNum, type)) %>%
    mutate(expNum=expNum)
}

runLMExp <- function(modelFile, modelName, alpha, lambda, theta, nUtterances, nComparisons, expNum) {
  # Setup
  modelStr <- getModelFile(modelFile)
  runFn <- createRunFn(modelStr)
  runExp_ <- runExperimentFn(runFn, modelName, alpha, lambda, theta, nUtterances)
  # Simulation data frame
  df_run <- runExp_(expNum)
  # RSA LM Data
  # (1) All utterances as a string
  # (2) All utterances no spaces
  allUtterances <- paste(df_run$currUtterance, collapse= " ")
  allUttarencesNoSpaces <- gsub(" ", "", allUtterances)
  df_bigrams <- buildBigramsDf(allUttarencesNoSpaces) %>%
    mutate(expNum=expNum,
           runNum=0,
           type='RSA')
  
  # Random LM Data
  # (3) All utterances no X (UID cues)
  # (4) Number of UID cues
  # (5) Non UID character as vector
  # (6) All character as vector (same as df_sim$currUtterance)
  allUtterancesXsRemoved <- gsub("X",'', allUttarencesNoSpaces)
  nXs <- countCharOccurrences('X', allUtterances)
  noXsChars <- strsplit(allUtterancesXsRemoved, split=" ")[[1]]
  chars <- strsplit(allUtterances, split=" ")[[1]]
  
  df_lm_sims <- buildRandomLMs(df_bigrams, allUtterancesXsRemoved, nXs, nComparisons, expNum)
  mergeLMsUtterances(df_run, df_lm_sims, expNum)
}
```

Run simulations
```{r run-sims, warning=FALSE, message=FALSE, echo=FALSE}
nSims <- 50
exps <- seq(1, nSims)
dfAll <- data.frame()
for (expNum in exps) {
  cat(expNum, ' ')
  df_curr <- runLMExp(plot3NoiseModelFile, "S4", 6, 1, 0.2, 500, 30, expNum)
  dfAll <- rbind(dfAll, df_curr)
}
```

```{r summarise-data, warning=FALSE, message=FALSE, echo=FALSE}
dfAllSummary <- dfAll %>%
  mutate(info=-log2(bigramProb)) %>%
  group_by(expNum, runNum, type) %>%
  summarise(avgInfo=median(info),
            s2Info=sd(info)^2)
```


# Simulations

Plot first 10 simulations. Green dot is the RSA `informativity` mean and variance measures. Red dots are the shuffled cue language model variance and informativity measures. While there is consistent variation between the experimental discourses the `RSA` ordering are consistently among the lowest variance.

```{r plot-sims, warning=FALSE, message=FALSE, echo=FALSE}
dfAllSummary %>%
  filter(expNum < 13) %>%
  ggplot(aes(x=avgInfo, y=s2Info, col=type)) +
    geom_point(alpha=0.75, size=2) +
    # geom_rug(alpha=0.5) +
    xlab("Corpora surprisal average (median)") +
    ylab("Corpora surprisal variance") +
    facet_wrap(~expNum, ncol=4) +
    ggtitle("RSA vs random cue orderings surprisal variance and expectation.") +
    theme_few() +
    theme(plot.title = element_text(hjust = 0.5))
```


Compare RSA cue ordering variance vs average cue ordering variance for the shuffled models. Again, RSA consistently produces lower variance corpora.
```{r plot2-variance, warning=FALSE, message=FALSE, echo=FALSE}
dfAllSummary2 <- dfAllSummary %>%
  group_by(expNum, type) %>%
  summarise(avgS2=mean(s2Info)) %>%
  spread(type, avgS2)

limMaxDiff <- 1 - max(max(dfAllSummary2$RSA), max(dfAllSummary2$random))
limMin <- min(min(dfAllSummary2$RSA), min(dfAllSummary2$random)) - 0.025
lmUsed <- limMin - limMaxDiff

dfAllSummary2 %>%
  ggplot(aes(x=random, RSA)) +
    geom_point(size=3, shape=21, stroke=1, col='white', fill='blue', alpha=0.5) +
    ylim(lmUsed, 1) +
    xlim(lmUsed, 1) +
    ylab("Average LM information variance\nRSA cue position") +
    xlab("Average LM information variance\nRandom cue position") +
    geom_abline(slope=1, intercept=0, lty=2, alpha=0.7) +
    theme_few() +
    ggtitle(paste("Noisy-RSA corpora minimize information variance\nn=", nSims, "simulations")) +
    theme(plot.title = element_text(hjust = 0.5))
  
```

We can test this in a mixed-effects model with `experiment`- and `shuffled number` random effets. We're interested in whether the type of ordering (`RSA` vs `random`) predicts variance.

`info variance ~ runType (1|experiment) + (1|run number)`
```{r lmer-model, warning=FALSE, message=FALSE, echo=FALSE}
dfAllSummary <- dfAll %>%
  mutate(info=-log2(bigramProb)) %>%
  group_by(expNum, runNum, type) %>%
  summarise(avgInfo=mean(info),
            s2Info=sd(info)^2)

summary(lmer(s2Info~as.factor(type)+(1|expNum)+(1|runNum), data=dfAllSummary))
```
RSA orderings have typically ~-0.05 bits less variance.
